{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjH3FDZVZ0Uc",
        "outputId": "74ec2c89-1255-4ec7-c763-2da0b5ba4647"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# CELL 1: Mount Google Drive and Import Libraries\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import zipfile\n",
        "from PIL import Image\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 2: Define Paths and Constants\n",
        "# ADJUST THESE PATHS\n",
        "MODEL_PATH = '/content/drive/MyDrive/CornLeaf_EfficientNetv2b2.keras' # Path to saved model\n",
        "TEST_ZIP_PATH = '/content/drive/MyDrive/CS180 Project Test Sets/corn_test.zip'\n",
        "EXTRACT_TO_PATH = '/content/corn_test_extracted/' # Temporary path to extract test images\n",
        "TEST_IMAGE_FOLDER = os.path.join(EXTRACT_TO_PATH, 'corn_test') # Path to the folder containing test images after extraction\n",
        "\n",
        "# Model input image dimension\n",
        "IMG_WIDTH = 260\n",
        "IMG_HEIGHT = 260\n",
        "CLASS_NAMES = ['Blight', 'Common_Rust', 'Gray_Leaf_Spot', 'Healthy'] # From training"
      ],
      "metadata": {
        "id": "aGpNjMxrZ_K9"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 3: Unzip the Test Set\n",
        "if os.path.exists(TEST_IMAGE_FOLDER):\n",
        "    print(f\"Test images folder already exists at {TEST_IMAGE_FOLDER}. Skipping unzip.\")\n",
        "else:\n",
        "    if os.path.exists(EXTRACT_TO_PATH):\n",
        "        # Clean up previous extraction if it exists but not the specific folder\n",
        "        import shutil\n",
        "        shutil.rmtree(EXTRACT_TO_PATH)\n",
        "    os.makedirs(EXTRACT_TO_PATH, exist_ok=True)\n",
        "    with zipfile.ZipFile(TEST_ZIP_PATH, 'r') as zip_ref:\n",
        "        zip_ref.extractall(EXTRACT_TO_PATH)\n",
        "    print(f\"Test images extracted to {TEST_IMAGE_FOLDER}\")\n",
        "\n",
        "# Verify extraction\n",
        "if os.path.exists(TEST_IMAGE_FOLDER):\n",
        "    print(f\"Found {len(os.listdir(TEST_IMAGE_FOLDER))} files in {TEST_IMAGE_FOLDER}\")\n",
        "else:\n",
        "    print(f\"Error: {TEST_IMAGE_FOLDER} not found after attempting to extract. Please check TEST_ZIP_PATH and extraction.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YjvxqnL6aA5F",
        "outputId": "e5cd2ce8-91ee-45ec-f7a5-b4be7744ce32"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test images folder already exists at /content/corn_test_extracted/corn_test. Skipping unzip.\n",
            "Found 838 files in /content/corn_test_extracted/corn_test\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 4: Load the Trained Model\n",
        "try:\n",
        "    model = tf.keras.models.load_model(MODEL_PATH)\n",
        "    print(\"Model loaded successfully.\")\n",
        "    model.summary() # Verify model structure\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model: {e}\")\n",
        "    # Add fallback or error handling if necessary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "id": "5J9X2PKNaCDy",
        "outputId": "24e03e43-4b25-47dd-cc7a-148ca1eeb314"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m260\u001b[0m, \u001b[38;5;34m260\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ data_augmentation (\u001b[38;5;33mSequential\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m260\u001b[0m, \u001b[38;5;34m260\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ efficientnetv2-b2 (\u001b[38;5;33mFunctional\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1408\u001b[0m)           │     \u001b[38;5;34m8,769,374\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_norm_top                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1408\u001b[0m)           │         \u001b[38;5;34m5,632\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ top_dropout (\u001b[38;5;33mDropout\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1408\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ output_layer (\u001b[38;5;33mDense\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)              │         \u001b[38;5;34m5,636\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">260</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">260</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ data_augmentation (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">260</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">260</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ efficientnetv2-b2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1408</span>)           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">8,769,374</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_norm_top                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1408</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">5,632</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ top_dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1408</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ output_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">5,636</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m15,376,396\u001b[0m (58.66 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">15,376,396</span> (58.66 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,297,876\u001b[0m (12.58 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,297,876</span> (12.58 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m5,482,766\u001b[0m (20.92 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,482,766</span> (20.92 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m6,595,754\u001b[0m (25.16 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,595,754</span> (25.16 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 5: Preprocessing Function for Test Images\n",
        "def preprocess_test_image(image_path):\n",
        "    try:\n",
        "        img = Image.open(image_path).convert('RGB')\n",
        "        img = img.resize((IMG_WIDTH, IMG_HEIGHT))\n",
        "        img_array = tf.keras.utils.img_to_array(img)\n",
        "        # EfficientNetV2B2 expects inputs in [0, 255] or uses its own preprocess_input.\n",
        "        # Our training pipeline used data_augmentation directly on [0, 255] images.\n",
        "        # So, for prediction, we just need to match the shape.\n",
        "        # No explicit tf.keras.applications.efficientnet_v2.preprocess_input was in our training for the base model call.\n",
        "        img_array = tf.expand_dims(img_array, 0) # Create a batch\n",
        "        return img_array\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing image {image_path}: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "-p46W-BQaDkV"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 6: Generate Predictions\n",
        "predictions_list = []\n",
        "test_image_files = sorted(os.listdir(TEST_IMAGE_FOLDER), key=lambda x: int(x.split('.')[0])) # Sort by number: 0.jpeg, 1.jpeg ...\n",
        "\n",
        "if not test_image_files:\n",
        "    print(f\"No image files found in {TEST_IMAGE_FOLDER}. Please check the path and extraction.\")\n",
        "else:\n",
        "    print(f\"Starting predictions for {len(test_image_files)} images...\")\n",
        "    for image_file in test_image_files:\n",
        "        if image_file.lower().endswith(('.jpeg', '.jpg', '.png')): # Check for common image extensions\n",
        "            image_path = os.path.join(TEST_IMAGE_FOLDER, image_file)\n",
        "            processed_image = preprocess_test_image(image_path)\n",
        "\n",
        "            if processed_image is not None:\n",
        "                try:\n",
        "                    prediction_probs = model.predict(processed_image, verbose=0)\n",
        "                    predicted_class_index = np.argmax(prediction_probs[0])\n",
        "                    predicted_class_name = CLASS_NAMES[predicted_class_index]\n",
        "                    predictions_list.append({'id': image_file, 'label': predicted_class_name})\n",
        "                except Exception as e:\n",
        "                    print(f\"Error predicting for image {image_file}: {e}\")\n",
        "                    predictions_list.append({'id': image_file, 'label': 'Error'}) # Or a default placeholder\n",
        "            else:\n",
        "                predictions_list.append({'id': image_file, 'label': 'PreprocessingError'})\n",
        "\n",
        "\n",
        "    print(f\"Finished predictions. {len(predictions_list)} predictions made.\")\n",
        "\n",
        "# Create DataFrame and save to CSV\n",
        "if predictions_list:\n",
        "    predictions_df = pd.DataFrame(predictions_list)\n",
        "    CSV_OUTPUT_PATH = '/content/drive/MyDrive/predictions.csv' # Save to your Drive\n",
        "    predictions_df.to_csv(CSV_OUTPUT_PATH, index=False)\n",
        "    print(f\"Predictions saved to {CSV_OUTPUT_PATH}\")\n",
        "\n",
        "    # Display first few predictions\n",
        "    print(\"\\nFirst 5 predictions:\")\n",
        "    print(predictions_df.head())\n",
        "else:\n",
        "    print(\"No predictions were made.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWQi_g2maE0S",
        "outputId": "1c2a8989-6ef9-4279-e6b5-bcbda5e3b726"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting predictions for 838 images...\n",
            "Finished predictions. 838 predictions made.\n",
            "Predictions saved to /content/drive/MyDrive/predictions.csv\n",
            "\n",
            "First 5 predictions:\n",
            "       id    label\n",
            "0  0.jpeg  Healthy\n",
            "1  1.jpeg  Healthy\n",
            "2  2.jpeg  Healthy\n",
            "3  3.jpeg  Healthy\n",
            "4  4.jpeg  Healthy\n"
          ]
        }
      ]
    }
  ]
}